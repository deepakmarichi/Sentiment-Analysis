{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\deepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\deepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import data and libraries\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2308, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_excel('Reviews.xlsx')\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Removing words starting with @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_rev = []\n",
    "# for text in reviews['review']:\n",
    "#     new_rev.append(\" \".join(filter(lambda x:x[0]!='@', text.split())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_rev[1967]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews['cleaned review'] = new_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_wordnet_pos(pos_tag):\n",
    "#     if pos_tag.startswith('J'):\n",
    "#         return wordnet.ADJ\n",
    "#     elif pos_tag.startswith('V'):\n",
    "#         return wordnet.VERB\n",
    "#     elif pos_tag.startswith('N'):\n",
    "#         return wordnet.NOUN\n",
    "#     elif pos_tag.startswith('R'):\n",
    "#         return wordnet.ADV\n",
    "#     else:\n",
    "#         return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Function for data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(text):\n",
    "#     # lower text\n",
    "#     text = text.lower()\n",
    "    \n",
    "#     # tokenize text and remove puncutation\n",
    "#     text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    \n",
    "#     # remove words that contain numbers\n",
    "#     text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    \n",
    "#     # remove stop words\n",
    "#     STOPWORDS = stopwords.words('english')\n",
    "#     newStopWords = ['the', 'credit', 'replying', 'to', 'one', 'like', 'used', 'see','american','express','amex','company', 'card', 'business', 'year', 'many', 'point', 'use', 'item']\n",
    "#     STOPWORDS.extend(newStopWords)\n",
    "#     text = [x for x in text if x not in STOPWORDS]\n",
    "    \n",
    "#     # remove empty tokens\n",
    "#     text = [t for t in text if len(t) > 0]\n",
    "    \n",
    "#     # pos tag text\n",
    "#     pos_tags = pos_tag(text)\n",
    "    \n",
    "#     # lemmatize text\n",
    "#     text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "    \n",
    "#     # remove words with three letter\n",
    "#     text = [t for t in text if len(t) > 2]\n",
    "    \n",
    "#     # join all\n",
    "#     text = \" \".join(text)\n",
    "    \n",
    "#     return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text data\n",
    "# reviews[\"cleaned review\"] = reviews[\"cleaned review\"].apply(lambda x: clean_text(x))\n",
    "# reviews[\"cleaned review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Add sentiment anaylsis columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.927, 'pos': 0.073, 'compound': 0.296}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "reviews[\"Sentiments\"] = reviews[\"review\"].apply(lambda x: sia.polarity_scores(x))\n",
    "print(reviews[\"Sentiments\"][0])\n",
    "reviews = pd.concat([reviews.drop(['Sentiments'], axis=1), reviews['Sentiments'].apply(pd.Series)], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADER sentiment analysis works on % rules which are explained below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.293, 'pos': 0.707, 'compound': 0.7034}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentIntensityAnalyzer().polarity_scores('GREAT customer service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.328, 'pos': 0.672, 'compound': 0.6249}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentIntensityAnalyzer().polarity_scores('great customer service')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.121, 'pos': 0.879, 'compound': 0.807}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentIntensityAnalyzer().polarity_scores('free reward points!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.125, 'pos': 0.875, 'compound': 0.7906}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentIntensityAnalyzer().polarity_scores('free reward points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Degree modifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.556, 'pos': 0.444, 'compound': 0.4927}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentIntensityAnalyzer().polarity_scores('Amex card is extremely beneficial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentIntensityAnalyzer().polarity_scores('Amex card is beneficial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Polarity shift due to Conjunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.235, 'neu': 0.596, 'pos': 0.169, 'compound': -0.25}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentIntensityAnalyzer().polarity_scores('Great customer service but problem is many merchants donâ€™t take AMEX.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Catching Polarity Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.556, 'pos': 0.444, 'compound': 0.4927}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentIntensityAnalyzer().polarity_scores('Customer service is really good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.303, 'neu': 0.697, 'pos': 0.0, 'compound': -0.3843}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentIntensityAnalyzer().polarity_scores('Customer service is not really that good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Saving the sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>review</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Amex_Delta_Gold</td>\n",
       "      <td>I am little despoilment with american express ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Amex_Delta_Gold</td>\n",
       "      <td>Time is of the essence in business and handlin...</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.5562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Amex_Delta_Gold</td>\n",
       "      <td>I wanted to book 3 flights and use miles plus ...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Amex_Delta_Gold</td>\n",
       "      <td>We (my wife &amp; I) both have a card in our own n...</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.9638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Amex_Delta_Plat</td>\n",
       "      <td>Reward programe is not so great also if any di...</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.3316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2303</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>@AmericanExpress is there any @AskAmex credit ...</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2304</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>Replying to @AskAmexWhy does it require a phon...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2305</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>It was with great pleasure that I cancelled my...</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.7906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2306</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>Replying to @AskAmex @ReelMoney and @AmericanE...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.8481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2307</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>@AskAmex We're having issues syncing with Expe...</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.6300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2308 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               source                                             review  \\\n",
       "0     Amex_Delta_Gold  I am little despoilment with american express ...   \n",
       "1     Amex_Delta_Gold  Time is of the essence in business and handlin...   \n",
       "2     Amex_Delta_Gold  I wanted to book 3 flights and use miles plus ...   \n",
       "3     Amex_Delta_Gold  We (my wife & I) both have a card in our own n...   \n",
       "4     Amex_Delta_Plat  Reward programe is not so great also if any di...   \n",
       "...               ...                                                ...   \n",
       "2303          Twitter  @AmericanExpress is there any @AskAmex credit ...   \n",
       "2304          Twitter  Replying to @AskAmexWhy does it require a phon...   \n",
       "2305          Twitter  It was with great pleasure that I cancelled my...   \n",
       "2306          Twitter  Replying to @AskAmex @ReelMoney and @AmericanE...   \n",
       "2307          Twitter  @AskAmex We're having issues syncing with Expe...   \n",
       "\n",
       "        neg    neu    pos  compound  \n",
       "0     0.000  0.927  0.073    0.2960  \n",
       "1     0.079  0.719  0.202    0.5562  \n",
       "2     0.000  1.000  0.000    0.0000  \n",
       "3     0.018  0.711  0.272    0.9638  \n",
       "4     0.152  0.577  0.271    0.3316  \n",
       "...     ...    ...    ...       ...  \n",
       "2303  0.103  0.803  0.094   -0.1280  \n",
       "2304  0.000  1.000  0.000    0.0000  \n",
       "2305  0.222  0.451  0.326    0.7906  \n",
       "2306  0.000  0.809  0.191    0.8481  \n",
       "2307  0.101  0.700  0.199    0.6300  \n",
       "\n",
       "[2308 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive).\n",
    "\n",
    "* positive sentiment : (compound score >= 0.05)\n",
    "* neutral sentiment : (-0.05 < compound score < 0.05)\n",
    "* negative sentiment : (compound score <= -0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_excel('Raw_Score.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
